#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
TWSE sidecar updater (roll25_cache/):
- Uses schema file: roll25_cache/twse_schema.json (field mapping / URLs)
- Fetches:
  1) FMTQIK: trade value / close / change  (MUST succeed; otherwise fail)
  2) MI_5MINS_HIST: OHLC (high/low)        (May degrade; never guess)
- Maintains rolling cache: roll25_cache/roll25.json (max 25 trading days)
- Writes:
  - roll25_cache/roll25.json
  - roll25_cache/latest_report.json

Policy:
- If FMTQIK fetch/parse fails => sys.exit(1) (do NOT silently succeed)
- If OHLC missing => degrade mode=MISSING_OHLC, but still produce report (never guess)
- manifest.json is generated by workflow AFTER data commit (so it can pin correct DATA_SHA)
"""

from __future__ import annotations

import json
import os
import re
import subprocess
import sys
from dataclasses import dataclass
from datetime import datetime, date
from zoneinfo import ZoneInfo
from typing import Any, Dict, List, Optional, Tuple

import requests

CACHE_DIR = "roll25_cache"
SCHEMA_PATH = os.path.join(CACHE_DIR, "twse_schema.json")

ROLL25_PATH = os.path.join(CACHE_DIR, "roll25.json")
LATEST_REPORT_PATH = os.path.join(CACHE_DIR, "latest_report.json")

LOOKBACK_TARGET = 20
STORE_CAP = 25


# ----------------- helpers -----------------

def _load_schema() -> Dict[str, Any]:
    if not os.path.exists(SCHEMA_PATH):
        raise RuntimeError(f"Missing schema file: {SCHEMA_PATH}")
    with open(SCHEMA_PATH, "r", encoding="utf-8") as f:
        return json.load(f)

def _tz(schema: Dict[str, Any]) -> ZoneInfo:
    tz = schema.get("timezone", "Asia/Taipei")
    return ZoneInfo(tz)

def _now_tz(tz: ZoneInfo) -> datetime:
    return datetime.now(tz=tz)

def _today_tz(tz: ZoneInfo) -> date:
    return _now_tz(tz).date()

def _is_weekend(d: date) -> bool:
    return d.weekday() >= 5

def _safe_float(x: Any) -> Optional[float]:
    if x is None:
        return None
    if isinstance(x, (int, float)):
        return float(x)
    s = str(x).strip()
    if s == "" or s.upper() == "NA" or s.lower() == "null":
        return None
    s = s.replace(",", "")
    try:
        return float(s)
    except Exception:
        return None

def _safe_int(x: Any) -> Optional[int]:
    f = _safe_float(x)
    if f is None:
        return None
    try:
        return int(round(f))
    except Exception:
        return None

def _roc_slash_to_iso(roc: str) -> Optional[str]:
    m = re.match(r"^(\d{2,3})/(\d{1,2})/(\d{1,2})$", roc.strip())
    if not m:
        return None
    y = int(m.group(1)) + 1911
    mo = int(m.group(2))
    da = int(m.group(3))
    try:
        return date(y, mo, da).isoformat()
    except Exception:
        return None

def _roc_compact_to_iso(compact: str) -> Optional[str]:
    s = compact.strip()
    if not re.match(r"^\d{7}$", s):
        return None
    try:
        roc_y = int(s[:3])
        mo = int(s[3:5])
        da = int(s[5:7])
        y = roc_y + 1911
        return date(y, mo, da).isoformat()
    except Exception:
        return None

def _parse_date_any(v: Any) -> Optional[str]:
    if v is None:
        return None
    if isinstance(v, (date, datetime)):
        return v.date().isoformat() if isinstance(v, datetime) else v.isoformat()
    s = str(v).strip()
    if s == "":
        return None
    if re.match(r"^\d{4}-\d{2}-\d{2}$", s):
        return s
    m = re.match(r"^(\d{4})/(\d{1,2})/(\d{1,2})$", s)
    if m:
        try:
            return date(int(m.group(1)), int(m.group(2)), int(m.group(3))).isoformat()
        except Exception:
            return None
    iso = _roc_slash_to_iso(s)
    if iso:
        return iso
    iso2 = _roc_compact_to_iso(s)
    if iso2:
        return iso2
    return None

def _http_get_json(url: str, timeout: int = 25) -> Any:
    headers = {"Accept": "application/json", "User-Agent": "twse-sidecar/1.5 (+github-actions)"}
    r = requests.get(url, headers=headers, timeout=timeout)
    r.raise_for_status()
    return r.json()

def _ensure_cache_dir() -> None:
    os.makedirs(CACHE_DIR, exist_ok=True)

def _read_json_file(path: str, default: Any) -> Any:
    if not os.path.exists(path):
        return default
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return default

def _write_json_file(path: str, obj: Any) -> None:
    with open(path, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2, sort_keys=False)

def _unwrap_to_rows(payload: Any) -> List[Dict[str, Any]]:
    if isinstance(payload, list):
        return [x for x in payload if isinstance(x, dict)]
    if isinstance(payload, dict):
        for k in ("data", "result", "records", "items", "aaData", "dataset"):
            v = payload.get(k)
            if isinstance(v, list):
                return [x for x in v if isinstance(x, dict)]
        if all(not isinstance(v, list) for v in payload.values()):
            return [payload]
    return []

def _pick_first_key(row: Dict[str, Any], keys: List[str]) -> Any:
    for k in keys:
        if k in row:
            return row.get(k)
    return None

def _diag_payload(name: str, payload: Any) -> None:
    print(f"[DIAG] {name}: type={type(payload).__name__}")
    if isinstance(payload, dict):
        print(f"[DIAG] {name}: top_keys={list(payload.keys())[:40]}")
    rows = _unwrap_to_rows(payload)
    print(f"[DIAG] {name}: unwrapped_rows={len(rows)}")
    if rows:
        print(f"[DIAG] {name}: row0_keys={list(rows[0].keys())[:40]}")

def _avg(xs: List[float]) -> Optional[float]:
    return (sum(xs) / len(xs)) if xs else None

def _calc_amplitude_pct(high: float, low: float, prev_close: float) -> Optional[float]:
    if prev_close == 0:
        return None
    return (high - low) / prev_close * 100.0


# ----------------- parsing -----------------

@dataclass
class FmtRow:
    date: str
    trade_value: Optional[int]
    close: Optional[float]
    change: Optional[float]

def _parse_fmtqik(payload: Any, schema: Dict[str, Any]) -> List[FmtRow]:
    s = schema["fmtqik"]
    rows = _unwrap_to_rows(payload)
    out: List[FmtRow] = []
    for r in rows:
        d = _parse_date_any(_pick_first_key(r, s["date_keys"]))
        if not d:
            continue
        tv = _safe_int(_pick_first_key(r, s["trade_value_keys"]))
        close = _safe_float(_pick_first_key(r, s["close_keys"]))
        chg = _safe_float(_pick_first_key(r, s["change_keys"]))
        out.append(FmtRow(d, tv, close, chg))
    out.sort(key=lambda x: x.date)
    return out

@dataclass
class OhlcRow:
    date: str
    high: Optional[float]
    low: Optional[float]
    close: Optional[float]

def _parse_ohlc(payload: Any, schema: Dict[str, Any]) -> List[OhlcRow]:
    s = schema["mi_5mins_hist"]
    rows = _unwrap_to_rows(payload)
    out: List[OhlcRow] = []
    for r in rows:
        d = _parse_date_any(_pick_first_key(r, s["date_keys"]))
        if not d:
            continue
        high = _safe_float(_pick_first_key(r, s["high_keys"]))
        low = _safe_float(_pick_first_key(r, s["low_keys"]))
        close = _safe_float(_pick_first_key(r, s["close_keys"]))
        out.append(OhlcRow(d, high, low, close))
    out.sort(key=lambda x: x.date)
    return out


# ----------------- cache logic -----------------

def _merge_roll(existing: List[Dict[str, Any]], new_items: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], bool]:
    m: Dict[str, Dict[str, Any]] = {}
    for it in existing:
        if isinstance(it, dict) and "date" in it:
            m[str(it["date"])] = it
    for it in new_items:
        if isinstance(it, dict) and "date" in it:
            m[str(it["date"])] = it

    merged = list(m.values())
    merged.sort(key=lambda x: str(x.get("date", "")), reverse=True)
    merged = merged[:STORE_CAP]
    dates = [str(x.get("date", "")) for x in merged]
    dedupe_ok = (len(dates) == len(set(dates)))
    return merged, dedupe_ok

def _latest_date(dates: List[str]) -> Optional[str]:
    return max(dates) if dates else None

def _pick_used_date(today: date, fmt_dates: List[str]) -> Tuple[str, str]:
    if not fmt_dates:
        return ("NA", "FMTQIK_EMPTY")
    today_iso = today.isoformat()
    if _is_weekend(today):
        return (_latest_date(fmt_dates) or fmt_dates[-1], "NON_TRADING_DAY")
    if today_iso not in fmt_dates:
        return (_latest_date(fmt_dates) or fmt_dates[-1], "DATA_NOT_UPDATED")
    return (today_iso, "OK_TODAY")

def _extract_lookback(roll: List[Dict[str, Any]], used_date: str) -> List[Dict[str, Any]]:
    eligible = [r for r in roll if isinstance(r, dict) and str(r.get("date", "")) <= used_date]
    eligible.sort(key=lambda x: str(x.get("date", "")), reverse=True)
    return eligible[:LOOKBACK_TARGET]

def _find_dminus1(roll: List[Dict[str, Any]], used_date: str) -> Optional[Dict[str, Any]]:
    eligible = [r for r in roll if isinstance(r, dict) and str(r.get("date", "")) < used_date]
    if not eligible:
        return None
    eligible.sort(key=lambda x: str(x.get("date", "")), reverse=True)
    return eligible[0]


# ----------------- main -----------------

def main() -> None:
    schema = _load_schema()
    tz = _tz(schema)

    _ensure_cache_dir()
    existing_roll = _read_json_file(ROLL25_PATH, default=[])
    if not isinstance(existing_roll, list):
        existing_roll = []

    # Fetch (FMTQIK MUST succeed)
    try:
        fmt_raw = _http_get_json(schema["fmtqik"]["url"])
    except Exception as e:
        print(f"[ERROR] FMTQIK fetch failed: {e}")
        sys.exit(1)

    # Fetch OHLC (may degrade)
    try:
        ohlc_raw = _http_get_json(schema["mi_5mins_hist"]["url"])
    except Exception as e:
        print(f"[WARN] MI_5MINS_HIST fetch failed: {e} (degrade to MISSING_OHLC)")
        ohlc_raw = []

    fmt_rows = _parse_fmtqik(fmt_raw, schema)
    ohlc_rows = _parse_ohlc(ohlc_raw, schema)

    if not fmt_rows:
        print("[ERROR] FMTQIK returned no usable rows/dates after parsing.")
        _diag_payload("FMTQIK", fmt_raw)
        sys.exit(2)

    fmt_by_date = {x.date: x for x in fmt_rows}
    ohlc_by_date = {x.date: x for x in ohlc_rows}

    fmt_dates = sorted(fmt_by_date.keys())
    today = _today_tz(tz)

    used_date, tag = _pick_used_date(today, fmt_dates)
    if used_date == "NA":
        print("[ERROR] UsedDate could not be determined.")
        _diag_payload("FMTQIK", fmt_raw)
        sys.exit(3)

    fmt_used = fmt_by_date.get(used_date)
    ohlc_used = ohlc_by_date.get(used_date)

    # Strict OHLC ok
    ohlc_ok = bool(ohlc_used and ohlc_used.high is not None and ohlc_used.low is not None)
    mode = "FULL" if ohlc_ok else "MISSING_OHLC"
    ohlc_status = "OK" if ohlc_ok else "MISSING"

    # Build cache item
    close = fmt_used.close if fmt_used else (ohlc_used.close if ohlc_used else None)
    change = fmt_used.change if fmt_used else None
    trade_value = fmt_used.trade_value if fmt_used else None
    high = ohlc_used.high if ohlc_used else None
    low = ohlc_used.low if ohlc_used else None

    cache_item = {
        "date": used_date,
        "close": close,
        "change": change,
        "trade_value": trade_value,
        "high": high,
        "low": low
    }

    merged_roll, dedupe_ok = _merge_roll(existing_roll, [cache_item])

    lookback = _extract_lookback(merged_roll, used_date)
    n_actual = len(lookback)
    oldest = lookback[-1]["date"] if lookback else "NA"

    # Freshness (oldest >45 days => unreliable)
    freshness_ok = True
    if n_actual > 0:
        try:
            oldest_dt = datetime.fromisoformat(str(oldest)).date()
            if (today - oldest_dt).days > 45:
                freshness_ok = False
        except Exception:
            freshness_ok = False

    dminus1 = _find_dminus1(merged_roll, used_date)
    used_dminus1 = dminus1["date"] if dminus1 else "NA"
    prev_close = _safe_float(dminus1.get("close")) if dminus1 else None

    today_close = _safe_float(close)
    today_trade_value = _safe_float(trade_value)
    today_change = _safe_float(change)

    pct_change = None
    if today_close is not None and prev_close is not None and prev_close != 0:
        pct_change = (today_close - prev_close) / prev_close * 100.0

    prior_days = lookback[1:] if n_actual >= 2 else []
    prior_tv = [_safe_float(x.get("trade_value")) for x in prior_days]
    prior_tv = [x for x in prior_tv if x is not None]

    volume_mult = None
    if today_trade_value is not None and len(prior_tv) >= 9:
        avg_tv = _avg(prior_tv[:LOOKBACK_TARGET - 1])
        if avg_tv and avg_tv != 0:
            volume_mult = today_trade_value / avg_tv

    amplitude_pct = None
    vol_mult = None
    if ohlc_ok and prev_close is not None and prev_close != 0:
        h = _safe_float(high)
        l = _safe_float(low)
        if h is not None and l is not None:
            amplitude_pct = _calc_amplitude_pct(h, l, prev_close)

            prior_amp: List[float] = []
            for x in prior_days:
                d = str(x.get("date", ""))
                hh = _safe_float(x.get("high"))
                ll = _safe_float(x.get("low"))
                dm1 = _find_dminus1(merged_roll, d)
                pc = _safe_float(dm1.get("close")) if dm1 else None
                if hh is None or ll is None or pc is None or pc == 0:
                    continue
                amp = _calc_amplitude_pct(hh, ll, pc)
                if amp is not None:
                    prior_amp.append(amp)

            if amplitude_pct is not None and len(prior_amp) >= 9:
                avg_amp = _avg(prior_amp[:LOOKBACK_TARGET - 1])
                if avg_amp and avg_amp != 0:
                    vol_mult = amplitude_pct / avg_amp

    is_down_day = (pct_change is not None and pct_change < 0) or (today_change is not None and today_change < 0)

    closes = []
    for x in lookback:
        c = _safe_float(x.get("close"))
        if c is not None:
            closes.append(c)

    new_low = False
    if today_close is not None and closes:
        new_low = (today_close <= min(closes))

    consecutive_break = False
    if dminus1 is not None:
        d1_date = str(dminus1.get("date"))
        d1_close = _safe_float(dminus1.get("close"))
        if d1_close is not None:
            d1_lookback = _extract_lookback(merged_roll, d1_date)
            d1_closes = [_safe_float(x.get("close")) for x in d1_lookback]
            d1_closes = [x for x in d1_closes if x is not None]
            if d1_closes:
                d1_new_low = (d1_close <= min(d1_closes))
                if new_low and d1_new_low:
                    consecutive_break = True

    # Risk logic
    risk_level = "未知"
    signal_text = ""

    if not freshness_ok:
        risk_level = "未知（資料不可靠）"
        signal_text = "資料過舊：lookback 最舊一筆距今 >45 天"
    elif n_actual < 10:
        risk_level = "未知（資料不足）"
        signal_text = "可得交易日數 <10，倍數不計算"
    else:
        if mode == "FULL":
            if is_down_day and (volume_mult is not None and volume_mult >= 1.2) and (vol_mult is not None and vol_mult >= 1.2):
                if (volume_mult >= 1.5) and (vol_mult >= 1.5) and (pct_change is not None and pct_change <= -1.5):
                    risk_level = "高"
                elif (pct_change is not None and pct_change <= -1.0):
                    risk_level = "中"
                else:
                    risk_level = "低"
                signal_text = "去槓桿風險上升（放量下跌 + 振幅放大）"
            else:
                risk_level = "低"
                signal_text = "未觸發 A) 規則"
        else:
            # Degraded: do NOT pretend; bias to unknown (possible miss)
            if is_down_day and (volume_mult is not None and volume_mult >= 1.3) and (pct_change is not None and pct_change <= -1.2):
                risk_level = "中"
                signal_text = "代理警訊：放量下跌；OHLC缺失可能漏報踩踏"
            else:
                risk_level = "未知（OHLC缺失，可能漏報）"
                signal_text = "OHLC缺失，無法套用完整模式（可能漏報）"

    prefix = ""
    if tag == "NON_TRADING_DAY":
        prefix = "今日非交易日；"
    elif tag == "DATA_NOT_UPDATED":
        prefix = "今日資料未更新；"

    if mode == "MISSING_OHLC":
        summary = f"{prefix}UsedDate={used_date}：{signal_text}；風險等級={risk_level}"
    else:
        summary = f"{prefix}UsedDate={used_date}：{signal_text}；風險等級={risk_level}"

    def _rnd(x: Optional[float], nd: int) -> Optional[float]:
        return None if x is None else round(float(x), nd)

    numbers = {
        "UsedDate": used_date,
        "Close": _rnd(today_close, 2),
        "PctChange": _rnd(pct_change, 3),
        "TradeValue": _safe_int(today_trade_value),
        "VolumeMultiplier": _rnd(volume_mult, 3),
        "AmplitudePct": _rnd(amplitude_pct, 3),
        "VolMultiplier": _rnd(vol_mult, 3)
    }

    signal = {
        "DownDay": bool(is_down_day) if (pct_change is not None or today_change is not None) else None,
        "VolumeAmplified": (volume_mult is not None and volume_mult >= 1.2) if volume_mult is not None else None,
        "VolAmplified": (vol_mult is not None and vol_mult >= 1.2) if vol_mult is not None else None,
        "NewLow_N": bool(new_low) if today_close is not None else None,
        "ConsecutiveBreak": bool(consecutive_break),
        "OhlcMissing": (not ohlc_ok),
    }

    if risk_level in ("中", "高"):
        if mode == "FULL" and "去槓桿風險上升" in signal_text:
            action = "先下調槓桿與部位曝險、提高保證金緩衝（例如提高現金比重/降低融資占比），並觀察下一個交易日是否延續破位與放量。"
        else:
            action = "先控槓桿與保證金緩衝；注意 OHLC 缺失可能漏報，待補齊後用完整模式重算。"
    else:
        action = "維持風險控管紀律（槓桿與保證金緩衝不惡化），持續每日觀察量能倍數、是否破位與資料完整性。"

    caveats_lines = [
        f"Sources: FMTQIK={schema['fmtqik']['url']} ; MI_5MINS_HIST={schema['mi_5mins_hist']['url']}",
    ]
    if not ohlc_ok:
        caveats_lines.append("OHLC: high 或 low 缺失（嚴格規則：不得硬猜；因此振幅倍數=NA且完整模式不觸發；可能漏報）")
    if dminus1 is None:
        caveats_lines.append("D-1: 昨收缺失（無法計算漲跌幅與振幅%）")

    caveats_lines.append(
        f"Mode={mode} | UsedDate={used_date} | UsedDminus1={used_dminus1} | "
        f"LookbackNTarget={LOOKBACK_TARGET} | LookbackNActual={n_actual} | "
        f"LookbackOldest={oldest} | OHLC={ohlc_status}"
    )

    latest_report = {
        "generated_at": _now_tz(tz).isoformat(),
        "timezone": str(tz),
        "summary": summary,
        "numbers": numbers,
        "signal": signal,
        "action": action,
        "caveats": "\n".join(caveats_lines),
        "cache_roll25": merged_roll,
        "tag": tag,
        "freshness_ok": freshness_ok,
        "risk_level": risk_level,
        "mode": mode,
        "ohlc_status": ohlc_status,
        "used_date": used_date,
        "used_dminus1": used_dminus1,
        "lookback_n_actual": n_actual,
        "lookback_oldest": oldest
    }

    # Write outputs (data only)
    _write_json_file(ROLL25_PATH, merged_roll)
    _write_json_file(LATEST_REPORT_PATH, latest_report)

    print("TWSE sidecar updated (data only):")
    print(f"  UsedDate={used_date}  Mode={mode}  Risk={risk_level}  LookbackNActual={n_actual}")
    print(f"  roll25_records={len(merged_roll)}  dedupe_ok={dedupe_ok}")


if __name__ == "__main__":
    main()